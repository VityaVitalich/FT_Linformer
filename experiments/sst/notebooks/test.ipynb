{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture \n",
    "# !pip install -U tokenizers\n",
    "# !pip install -U transformers\n",
    "# !pip install -U tokenizers\n",
    "# !pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "sys.path.append('../../../')\n",
    "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "from transformers.models.bert.modeling_linbert import BertForSequenceClassification, BertForMaskedLM\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "from fvcore.nn import FlopCountAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding, DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from transformers_modified.models.bert.tokenization_bert import BertTokenizer\n",
    "# BertModelForSequenceClassification\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'bert-base-cased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(model_id)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "seq_class_model = BertForSequenceClassification.from_pretrained(model_id, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearized\n"
     ]
    }
   ],
   "source": [
    "seq_class_model.linearize(128, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.tensor([[0, 15, 3, 4], [0, 1, 1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6627, -0.1643],\n",
       "        [ 0.7424, -0.1676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_class_model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:48<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 356.131806640625 +- 19.686272189282995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "seq_class_model.to(device)\n",
    "dummy_input = torch.zeros(16, 512, dtype=torch.long).to(device)\n",
    "\n",
    "# INIT LOGGERS\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "#GPU-WARM-UP\n",
    "for _ in range(10):\n",
    "    _ = seq_class_model(dummy_input)\n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "    for rep in tqdm(range(repetitions)):\n",
    "        starter.record()\n",
    "        _ = seq_class_model(input_ids=dummy_input)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print('mean {} +- {}'.format(mean_syn, std_syn))\n",
    "# 8 - 4.645040099620819 16 - 4.87 32 - 5.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearized\n"
     ]
    }
   ],
   "source": [
    "lin_model = copy.deepcopy(seq_class_model)\n",
    "lin_model.linearize(128, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 1000/1000 [04:41<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean linearized 276.56400245666504 +- 44.34218480368732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "lin_model.to(device)\n",
    "dummy_input = torch.zeros(64, 128, dtype=torch.long).to(device)\n",
    "\n",
    "# INIT LOGGERS\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 1000\n",
    "timings=np.zeros((repetitions,1))\n",
    "#GPU-WARM-UP\n",
    "for _ in range(10):\n",
    "    _ = lin_model(dummy_input)\n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "    for rep in tqdm(range(repetitions)):\n",
    "        starter.record()\n",
    "        _ = lin_model(input_ids=dummy_input)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print('mean linearized {} +- {}'.format(mean_syn, std_syn))\n",
    "# 8 - 4.908345495859782  16 - 4.972362136046092 64 - 6.3\n",
    "# mean linearized 6.310050270318985 +- 1.5480504170206077\n",
    "# mean 6.814684094429016 +- 2.0908597950889405\n",
    "\n",
    "# 128 mean linearized 10.194645232677459 +- 2.1097197328117305\n",
    "# 128 mean 10.301190483093261 +- 2.14933867617596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(tokenizer):\n",
    "    train_set = datasets.load_dataset('sst2', split='train').remove_columns(['idx'])\n",
    "    val_set = datasets.load_dataset('sst2', split='validation').remove_columns(['idx'])\n",
    "\n",
    "    dynamic_padding = True\n",
    "\n",
    "    def tokenize_func(examples):\n",
    "        return tokenizer(examples[\"sentence\"], max_length=128, truncation=True)\n",
    "\n",
    "    encoded_dataset_train = train_set.map(tokenize_func, batched=True)\n",
    "    encoded_dataset_test = val_set.map(tokenize_func, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "   # data_collator = DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15)\n",
    "    return encoded_dataset_train, encoded_dataset_test, data_collator\n",
    "\n",
    "def create_mlm_data(tokenizer):\n",
    "    train_set = datasets.load_dataset('imdb', split='train').remove_columns(['label'])\n",
    "    val_set = datasets.load_dataset('imdb', split='test').remove_columns(['label'])\n",
    "\n",
    "    def tokenize_func(examples):\n",
    "        return tokenizer(examples[\"text\"], max_length=128, padding='max_length', truncation=True)\n",
    "\n",
    "    encoded_dataset_train = train_set.map(tokenize_func, batched=True)\n",
    "    encoded_dataset_val = val_set.map(tokenize_func, batched=True)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer)\n",
    "\n",
    "    return encoded_dataset_train, encoded_dataset_val, data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb9c985645b49308353f4a2584fb709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00440d5e1b04454ca322010198b9be8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset_train, encoded_dataset_test, data_collator = create_data(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([29978, 17189,  9249,  5375,  3051,  1497,   675,   247,    76,\n",
       "           12]),\n",
       " array([ 3. ,  9.9, 16.8, 23.7, 30.6, 37.5, 44.4, 51.3, 58.2, 65.1, 72. ]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(list(map(len, encoded_dataset_train['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.952649631026445"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(map(len, encoded_dataset_train['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 27 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 1 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::masked_fill_ encountered 24 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::softmax encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11186505216\n",
      "linearized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 27 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 1 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::masked_fill_ encountered 24 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::softmax encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11035510272\n",
      "150994944\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "seq_len = 128\n",
    "k = 32\n",
    "bs = 1\n",
    "\n",
    "seq_class_model = seq_class_model.to(device)\n",
    "dummy_input = torch.ones(bs, seq_len, dtype=torch.long).to(device)\n",
    "\n",
    "default_flops = FlopCountAnalysis(seq_class_model, dummy_input) \n",
    "print(default_flops.total())\n",
    "\n",
    "lin_model = copy.deepcopy(seq_class_model)\n",
    "lin_model.linearize(seq_len, k)\n",
    "\n",
    "lin_model = lin_model.to(device)\n",
    "lin = FlopCountAnalysis(lin_model, dummy_input)\n",
    "print(lin.total())\n",
    "\n",
    "print(default_flops.total() - lin.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0136826426942045"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11186505216 / 11035510272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 27 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 1 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::masked_fill_ encountered 24 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::softmax encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "773890007040"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops.total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1062999/2290328016.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric('accuracy')\n"
     ]
    }
   ],
   "source": [
    "metric = datasets.load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\tpredictions, labels = eval_pred\n",
    "\tpredictions = np.argmax(predictions, axis=1)\n",
    "\treturn metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert-base-cased-sst2',\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.1,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    seed=42,\n",
    "    save_strategy = \"epoch\",\n",
    "    save_total_limit=5,\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"all\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=seq_class_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset_train,\n",
    "    eval_dataset=encoded_dataset_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2635' max='2635' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2635/2635 14:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.210278</td>\n",
       "      <td>0.913991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.263743</td>\n",
       "      <td>0.917431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.267314</td>\n",
       "      <td>0.917431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.294708</td>\n",
       "      <td>0.926606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.334369</td>\n",
       "      <td>0.920872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2635, training_loss=0.09690964642693015, metrics={'train_runtime': 882.1021, 'train_samples_per_second': 381.753, 'train_steps_per_second': 2.987, 'total_flos': 2.21503330843008e+16, 'train_loss': 0.09690964642693015, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.21028630435466766,\n",
       " 'eval_accuracy': 0.9139908256880734,\n",
       " 'eval_runtime': 0.7942,\n",
       " 'eval_samples_per_second': 1097.941,\n",
       " 'eval_steps_per_second': 8.814,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(encoded_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb30efe576af428fa9267855b0b62509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/217M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa5cf6cc1e74d8daca3befb2400d5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cb409d7b754ca1848f102e2ed3b91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/VityaVitalich/bert-base-cased-sst2/tree/main/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub('bert-base-cased-sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/VityaVitalich/bert-base-cased-sst2/commit/8bca97af90ae393e48881bea8254e4c1b498a540', commit_message='Upload tokenizer', commit_description='', oid='8bca97af90ae393e48881bea8254e4c1b498a540', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('bert-base-cased-sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4555cfd376c40118283a2786e71513d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900ade9e1dd74a38a944621ee14037be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d810bf2aa02647ed8cc7fe81b6fe7c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/8.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9d2029d70b4267a2b85f347645d9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/VityaVitalich/results/tree/main/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub('bert-tiny-sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_downloaded = BertForSequenceClassification.from_pretrained('VityaVitalich/bert-tiny-sst2', num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('VityaVitalich/bert-tiny-sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_downloaded,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset_train,\n",
    "    eval_dataset=encoded_dataset_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/7 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4771495461463928,\n",
       " 'eval_accuracy': 0.8279816513761468,\n",
       " 'eval_runtime': 0.2513,\n",
       " 'eval_samples_per_second': 3469.28,\n",
       " 'eval_steps_per_second': 27.85}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(encoded_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_class_model.bert = trainer.model.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0107,  0.0228,  0.1497,  ...,  0.0589,  0.1436,  0.0942],\n",
       "        [ 0.0464,  0.0290, -0.0227,  ...,  0.0387,  0.1120,  0.0134],\n",
       "        [-0.0765, -0.1274,  0.1461,  ...,  0.0350,  0.0740,  0.0756],\n",
       "        ...,\n",
       "        [ 0.1165, -0.1550, -0.0914,  ..., -0.0647, -0.0303,  0.0779],\n",
       "        [-0.0241,  0.1494,  0.1148,  ..., -0.0482,  0.1539, -0.0846],\n",
       "        [-0.0982, -0.0761,  0.0075,  ...,  0.1350, -0.0933,  0.1502]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_class_model.bert.encoder.layer[0].attention.self.E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/VityaVitalich/bert-tiny-sst2/commit/1536c4b2a95662057c5c34e1b0d3ca835446efa2', commit_message='Upload tokenizer', commit_description='', oid='1536c4b2a95662057c5c34e1b0d3ca835446efa2', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('bert-tiny-sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
